<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://jcpayne.github.io/aerial-survey-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jcpayne.github.io/aerial-survey-blog/" rel="alternate" type="text/html" /><updated>2021-07-30T18:03:34-05:00</updated><id>https://jcpayne.github.io/aerial-survey-blog/feed.xml</id><title type="html">AI for aerial surveys</title><subtitle>A project to develop an AI pipeline for aerial surveys</subtitle><entry><title type="html">AI for aerial surveys in the real world</title><link href="https://jcpayne.github.io/aerial-survey-blog/2021/07/31/AI-for-aerial-surveys-in-the-real-world.html" rel="alternate" type="text/html" title="AI for aerial surveys in the real world" /><published>2021-07-31T00:00:00-05:00</published><updated>2021-07-31T00:00:00-05:00</updated><id>https://jcpayne.github.io/aerial-survey-blog/2021/07/31/AI-for-aerial-surveys-in-the-real-world</id><author><name></name></author><summary type="html">By John Payne</summary></entry><entry><title type="html">Project overview</title><link href="https://jcpayne.github.io/aerial-survey-blog/2021/07/30/project-overview.html" rel="alternate" type="text/html" title="Project overview" /><published>2021-07-30T00:00:00-05:00</published><updated>2021-07-30T00:00:00-05:00</updated><id>https://jcpayne.github.io/aerial-survey-blog/2021/07/30/project-overview</id><author><name>John Payne</name></author><summary type="html">Overview of the project For decades, the standard way to do an aerial wildlife survey was to fly some sort of grid pattern and have human observers in an airplane count and photograph things that they saw. However, humans are expensive, they get tired, and they miss animals. With the advent of digital photography and drones, it is becoming easier to fly transects and take nearly continuous photographs. A moderate-sized survey can easily generate 100,000 images. The challenge then becomes figuring out how to detect objects in those images. In most wildlife surveys, at least 95% of the images are ‘empty,’ meaning they contain no objects of interest.</summary></entry><entry><title type="html">Classification Model</title><link href="https://jcpayne.github.io/aerial-survey-blog/2021/06/30/classification-model.html" rel="alternate" type="text/html" title="Classification Model" /><published>2021-06-30T00:00:00-05:00</published><updated>2021-06-30T00:00:00-05:00</updated><id>https://jcpayne.github.io/aerial-survey-blog/2021/06/30/classification-model</id><author><name></name></author><summary type="html">Adding a classification model as a preliminary filter In desperation, I decided to add a multi-label classification model as a preliminary filter. The idea was that if it was accurate, it could be used to eliminate most of the empty tiles, thereby greatly reducing the workload on the slow Detectron2 model. I chose a ResNet50 model from fastai, because a ResNet50 should be fast and fastai’s training environment is superb.</summary></entry><entry><title type="html">Slow Inference And Kazakhstan</title><link href="https://jcpayne.github.io/aerial-survey-blog/2021/03/26/slow-inference-and-kazakhstan.html" rel="alternate" type="text/html" title="Slow Inference And Kazakhstan" /><published>2021-03-26T00:00:00-05:00</published><updated>2021-03-26T00:00:00-05:00</updated><id>https://jcpayne.github.io/aerial-survey-blog/2021/03/26/slow-inference-and-kazakhstan</id><author><name></name></author><summary type="html">Slow inference causes a traffic jam I was getting excited that we were ready to do inference and to finally deploy the model. I took a deep dive into Azure DevOps, Kubernetes clusters and web endpoints, and got as far as running the model in a container (which required ‘registering’ the model, among other steps). I was discouraged to find that it ran far too slowly, and upgrading the container with a GPU proved too difficult and expensive. So I abandoned that approach and decided to stick with working on VMs for now.</summary></entry><entry><title type="html">Training And Sharing Results</title><link href="https://jcpayne.github.io/aerial-survey-blog/2021/02/11/training-and-sharing-results.html" rel="alternate" type="text/html" title="Training And Sharing Results" /><published>2021-02-11T00:00:00-06:00</published><updated>2021-02-11T00:00:00-06:00</updated><id>https://jcpayne.github.io/aerial-survey-blog/2021/02/11/training-and-sharing-results</id><author><name></name></author><summary type="html">Training the model Once the training and validation sets were created, the model customization was working, and I had learned how to use AML, I trained the model fairly hard on an AML cluster with 4 GPUs. The process did not go smoothly. Fixing learning rate problems The model didn’t improve much at first, and I realized that the learning rate was probably wrong. However, I didn’t have a good way to visualize results because the Detectron2 logger is self-contained and didn’t communicate with the AML cluster. To use AML’s visualizations properly, it would be necessary to intercept the Detectron2 messages on their way to its logger and share them with AML. I have since figured out how to do that, but at the time, I just wrote a bit of code to visualize both the output and the learning rate schedule once the run was finished. I then experimented quite actively with learning rate schedules. I found that I had misinterpreted some of Detectron2’s learning rate schedule parameters and the initial learning rate was too high. I eventually figured out where to aim (although it was a moving target, because the best rate dropped as the model improved), and training finally started going well. Dropping the ‘boma’ class and re-training In the US, a corral for livestock is usually made with fence posts and wire, but in subSaharan Africa, corrals are often made by piling up brush in a rough circle or rectangle. In Tanzania, those brush corrals are called ‘bomas’. Howard wanted to be able to identify bomas, and they were included in our object list. As the training proceeded, the model accuracy got up to 80-90% for most of the important classes, but there were still an unacceptable number of false-positives. Looking at the output, we quickly realized that most of the problem was in the ‘boma’ class. The model obviously recognized straight edges, but sometimes got the scale very wrong because at smaller scales, everything looks like a boma. We didn’t have a very big sample size of labeled bomas, and we decided that it was better to leave the class for another time. So we dropped it and retrained, starting by training the ROI heads with the rest of the model layers frozen. That fixed the false-positive problem and the accuracy got quite good. It’s currently above 95% for the common categories, with only 5 - 7% false-positives. The 05_aml_pipeline.ipynb notebook describes the training. TridentNet’s three heads TridentNet has an unusual feature from which its name is derived: the model has three separate heads that handle small, medium, and large objects, respectively. The TridentNet creators wrote that once the model was trained, it worked perfectly well to do inference using only the medium head, to save computation. We compared inference using all of the heads with inference using only the medium head and so far, I agree with their conclusion.</summary></entry></feed>