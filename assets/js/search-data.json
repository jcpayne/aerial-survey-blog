{
  
    
        "post0": {
            "title": "AI for aerial surveys in the real world",
            "content": "By John Payne . Overview . This blog is a narrative description of my efforts to develop an AI pipeline to recognize wildlife, livestock and other objects in aerial surveys. The work is my contribution to a project run by Howard Frederick, from the Tanzanian Wildlife Service. I initially thought it would be straightforward, but the real world contains all sorts of tradeoffs and it has been interesting and challenging to get as far as we have. I hope that readers may find some relief and humor from my story because they struggled with things that I did (or can’t believe how dumb I was), and may perhaps even find a snippet or two that is useful to them. . The code referred to is available in the aerial_survey_ai Github repository. . This project has been generously supported by Microsoft’s AIForEarth program (AI4E), which provided us with credits on Microsoft’s Azure cloud computing ecosystem, as well as good advice, technical support, and the opportunity to meet other grantees. I am very grateful for their support. . Introduction to the problem . For decades, the standard way to do an aerial wildlife survey was to fly some sort of grid pattern and have human observers in an airplane count and photograph things that they saw. However, humans are expensive, they get tired, and they miss animals. With the advent of digital photography and drones, it is becoming easier to fly transects and take nearly continuous photographs. A moderate-sized survey can easily generate 100,000 images. The challenge then becomes figuring out how to detect objects in those images. In most wildlife surveys, at least 95% of the images are ‘empty,’ meaning they contain no objects of interest. . In 2013, I joined Howard and two of his colleagues: Mike Norton-Griffiths (the project leader and author of the classic aerial survey textbook, Counting Animals) and Dana Munro Slaymaker, a digital survey and FLIR expert, on a survey of wildlife and livestock over 150,000 km2 of the Mongolian Gobi Desert. The survey generated more than 100,000 photographs, and it took a team of 6 people two months working full-time to go through them. The photographic analysis required a complicated lab setup with identical workstations and a back-end database to feed the annotators batches of images, track the progress of image annotation, compare annotators against each other, estimate false-positive and false-negative error rates by double-counting a fraction of the images, and so on. It was an expensive and complicated proposition. Although it will probably always be necessary to have humans in the loop, that experience helped to solidify our determination to reduce the amount of human work that is necessary by developing a machine learning pipeline for object detection in aerial images. . Conservation arguments over elephant numbers prompted Paul Allen’s Vulcan Inc. to fund the Great Elephant Census in 2016, which was a first attempt to do nearly simultaneous surveys of elephants across subSaharan Africa. Howard was one of the technical leads for the project. The census was successful, but the extreme difficulty of pulling it off highlighted the need for better methods. If surveys are to be used for monitoring wildlife (or anything else) it is important for them to be done regularly and often, and for the results to be available quickly. . Our goal is to make aerial surveys easier to do, cheaper to analyze, and ultimately more accurate. In particular, we would like to make them more accessible to government wildlife and agricultural departments in developing countries. We are working on aspects of that challenge; to be specific, Howard is designing inexpensive camera systems and associated equipment, doing ongoing surveys and training in southern and eastern Africa, and leading collaborations with a variety of people and organizations on many aspects of surveys, while I have been beavering away at using AI for object detection. Together, we are designing a workflow that takes data from the airplane on a dusty runway to the local wildlife department, to the cloud, through an AI pipeline, and back out in the form of numbers that can be analyzed statistically. . Understanding aerial surveys . Survey workflow . An aerial survey is a complicated beast. The goal is usually to obtain counts or density estimates, or possibly to measure indicators, for some objects of interest over a large area. Howard and I drew this diagram together the other day, trying to picture all of the components. The top band shows the overall survey workflow. The rest of the diagram shows roughly what is needed for just the object recognition box, which is only a small part of the total workflow. {width: 100%} . AI priorities are different for an aerial survey! . The aim of an aerial wildlife or livestock survey is to estimate the numbers and spatial distribution of animals that are usually very rare (sometimes present in &lt; 1% of all images), clumped together, and often half-hidden in forest or water environments. The precision and accuracy of the density estimates ultimately depend on a number of different sources of error, but one of the top sources of error is that surveys sample only a small fraction of the entire landscape. Therefore, any error in counting animals in the sampled areas will balloon when it is extrapolated to the whole landscape. . Therefore, the priorities of AI for a survey are different than the priorities of AI in other applications. The priorities in descending order of importance are: 1) Never fail to detect an animal that is present in an image. Surveys require a very low false-negative rate above all else. 2) A relatively low false-positive rate. If there are too many false-positives, then the model doesn’t save the human annotators much work. Ideally, the false-positive rate should be well below 5%. 3) Reasonable speed. Surveys generate a lot of images, and a model isn’t very useful if it doesn’t run fast enough. 4) Perhaps surprisingly, Accurate species identification is a relatively low priority. In most cases, there are few enough animals that humans will be scrutinizing every photograph that contains animals. The role of the model is primarily to sort through vast piles of empty images to find animals in the first place. In practice, AI models get really good at species identification anyway, and we don’t mind if rare species are occasionally misidentified. 5) Accurate instance counts. For some surveys (for example, counting nesting birds on a colony), accurate instance counts may be a high priority. But for us, correct instance counts are a relatively low priority because people will be examining most or all of the photographs that contain animals. Counting is always made difficult by the fact that animals often stand close together (particularly mothers and offspring) or pile on top of each other, and in our case, the tiling process described below makes counting more difficult because it’s hard to ensure that there is no duplication between object instances. .",
            "url": "https://jcpayne.github.io/aerial-survey-blog/2021/07/31/AI-for-aerial-surveys-in-the-real-world.html",
            "relUrl": "/2021/07/31/AI-for-aerial-surveys-in-the-real-world.html",
            "date": " • Jul 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Current status",
            "content": "Current status: ready for large batches . After months and months of work, the project is finally at the point where we can run large batches of images through the pipeline. . We have two working models: a heavy bounding-box model (TridentNet), and a much lighter and faster multi-label classification model (ResNet50 from fastai). | Both can be used as is; they both have high enough accuracy (&gt;90% for the most important categories) and low enough false-positive rates (&lt;5%) to be useful. | The classification model reduces the workload for the bounding box model by 95%. The multi-label classification model reduces the number of tiles that the big Detectron2 bounding-box model needs to check by almost 95% (&gt;98% reduction if you’re willing to ignore ‘uncertain’ tiles), and it is fast enough to be used in production. Both of those points are very good news, and it means we could even afford to throw in a random sample of ‘empty’ tiles for the big bounding-box model to double-check. | It may not reduce the workload for humans as much. Only 4.8% of the total tiles are classified as ‘uncertain’, meaning that the model doesn’t classify them as anything, not even ‘empty’. Unfortunately, despite their modest numbers, those ‘uncertain’ tiles are scattered very evenly across files, with the result that about 1/3 of the images are empty, 1/3 contain objects, and 1/3 are a mix of ‘empty’ and ‘uncertain’ tiles. That means that effectively, the multilabel classification model only gives us a 1/3 reduction in the number of full-size images that we need to check. Depending on how we design the pipeline and what we decide to double-check, that may mean that the workload for humans is not reduced as much as the workload for the Detectron2 model. | Our primary need at the moment is to find more training images. We will need to use the models to help with that process by churning through big piles of new images. | We need to develop an environment for human-in-the-loop work. We have been exploring AIDE and Howard has also begun to work with the team at WildMe. Keeping track of edits to annotations is a very complicated problem. | We need to improve the existing code. It needs to be made more modular, more stable, and to have some of the functions packaged properly, to make them easier to use by other people. -We need a more stable and solid pipeline. I have used the word ‘pipeline’ rather usely throughout this blog, but in reality we would like to move towards an automated Azure pipeline that makes it easier to pass data from one step to the next. | . In summary, the future is looking quite good. I look forward to seeing whether the big model can eliminate some of the ‘uncertain’ tiles and thereby reduce the number of fullsized images (although it could add new false-positives, too).  There are still quite a few categories where the number of training images was very low and the models are still very confused about them. I expect that as the models improve and that confusion is reduced, the models will get less unsure about whether a file is empty, which will help to shrink the number of ‘uncertain’ tiles. .",
            "url": "https://jcpayne.github.io/aerial-survey-blog/2021/07/30/current-status.html",
            "relUrl": "/2021/07/30/current-status.html",
            "date": " • Jul 30, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Creating a classification model",
            "content": "Adding a classification model as a preliminary filter . In desperation, I decided to add a multi-label classification model as a preliminary filter. The idea was that if it was accurate, it could be used to eliminate most of the empty tiles, thereby greatly reducing the workload on the slow Detectron2 model. I chose a ResNet50 model from fastai, because a ResNet50 should be fast and fastai’s training environment is superb. . To make a longish story short, I created a new VM, installed the fastai environment, ported the training images, annotations and model file to the new environment, converted the annotations to the multi-label classification format, and trained the model, as described in 06_fastai_model.ipynb. It soon reached an acceptable level of accuracy (about the same as the Detectron2 model) and so I ran an initial batch of test images through the model. To my horror, inference took 0.37 seconds per image. Several desperate hours later, thanks to the wonderful fastai community I discovered a hidden parameter whose default was cpu=True in the function that re-loaded a saved model. In other words, it wasn’t using the GPU! I changed that to cpu=False and Hey Presto! It ran 150 X faster. . Doing inference with the classifier model . It was time to put the classification model to a more serious test. I ran a batch of 20,000 full-size images from a study site that the model had never seen before. Because I hadn’t yet figured out how to pass in-memory tiles to a fastaidataloader, I wrote just over 1 million tiles (20,000 X 54) to disk. That tiling took nearly 12 hours, which is a problem, but I was very relieved to find that it took less than 2 hours to run 1 million tiles through the model on an Azure NC6_v3 machine (which has an Nvidia V100 GPU). Finally, we had a model that was fast enough to be really useful! But was it accurate enough? . Assessing a multi-label classification model. Surprising edge cases. . The standard ‘confusion table’ method for assessing the output of a single-label classification model does not work in the multi-label case. In a multi-label model, you never know whether an object was missed (not detected at all), or misclassified (detected, but classified as the wrong category). I haven’t found a good way to assess a multi-label model, but I wrote some code to try to separate missed from misclassified objects, as shown in the . In the process, I discovered two surprising edge cases: 1) Some tiles were not classified as anything. No category including the ‘empty’ category had a score that was higher than the detection threshold. I named these predictions ‘uncertain’. 2) A few tiles were classified as both empty and as containing an object. That wasn’t really a problem because we could just ignore the ‘empty’ classification in these cases. . Is fastai’s implementation of ‘empty’ a feature or a bug? . Clearly, the fastai model’s empty’ category is not just a remainder that is created when the model fails to find any object in an image; it’s actually a learned category itself. That architectural choice generates the strange edge cases that make counting results much harder. But is it a bug or a feature? . We examined the ‘uncertain’ tiles and discovered that in many cases, there were interesting objects in them; sometimes objects that were not in our list, and other times false-positives. Essentially, the ‘uncertain’ category is quite useful because it highlights images where ‘something is not quite right,’ and we therefore decided to treat it as a feature, not a bug. As our model improves (particularly for rare categories), we expect that the number of ‘uncertain’ predictions will shrink, but they will still highlight tiles that are worth examining more closely. .",
            "url": "https://jcpayne.github.io/aerial-survey-blog/2021/06/30/classification-model.html",
            "relUrl": "/2021/06/30/classification-model.html",
            "date": " • Jun 30, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Slow inference and a new application",
            "content": "Slow inference causes a traffic jam . I was getting excited that we were ready to do inference and to finally deploy the model. I took a deep dive into Azure DevOps, Kubernetes clusters and web endpoints, and got as far as running the model in a container (which required ‘registering’ the model, among other steps). I was discouraged to find that it ran far too slowly, and upgrading the container with a GPU proved too difficult and expensive. So I abandoned that approach and decided to stick with working on VMs for now. . But I had nagging doubts. How slow was the model, I wondered? And had I done something wrong? I had a series of exchanges with Yu Wuxin, the Detectron2 wizard at Facebook, in which I built a reproducible example, tested it on various Azure and Google machines, and finally concluded that it took 0.2 seconds per 800x800 image; a result that he eventually agreed was probably correct. . It was really dismaying to realize that if we started with 100,000 images from a survey, multiplied that by 54 tiles/image * 0.2 seconds/tile on an Azure NC_v3 machine that cost $13.50/hour, then we were looking at a cost between $1000 and $10,000 (depending on how many problems we could solve) to run the tiles through the model once. That was clearly unsustainable (at least, in our impoverished field, and certainly for most wildlife departments in developing countries). Something had to give! . Building a model for Kazakhstan . At this point, I took a break from inference problems and created a new model for a colleague of mine, Petra Kaczensky, who was doing drone surveys of wildlife in Kazakhstan. I followed most of the same steps as before (except I was smarter about being hyper-organized from the outset so there were fewer problems assembling training data), and used transfer learning to train a new head for the Tanzania Detectron model. Kazakhstan was an interesting challenge because the environment is much drier and wildlife is so scarce that very few images of the key species were available, so the training set was very small. We augmented pictures from Kazakhstan with some from Mongolia, which has some of the same species and a similar desert environment, and with some wildlife images from Tanzania. Despite the challenges, we fairly soon had decent results on the TridentNet model (OK that’s an exaggeration, it was actually a lot of work). . The inference speed roadblock remained, however, and it posed the same challenges in Kazakhstan as it did in Tanzania. .",
            "url": "https://jcpayne.github.io/aerial-survey-blog/2021/03/26/slow-inference-and-kazakhstan.html",
            "relUrl": "/2021/03/26/slow-inference-and-kazakhstan.html",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Training the model and sharing results",
            "content": "Training the model . Once the training and validation sets were created, the model customization was working, and I had learned how to use AML, I trained the model fairly hard on an AML cluster with 4 GPUs. The process did not go smoothly. . Fixing learning rate problems . The model didn’t improve much at first, and I realized that the learning rate was probably wrong. However, I didn’t have a good way to visualize results because the Detectron2 logger is self-contained and didn’t communicate with the AML cluster. To use AML’s visualizations properly, it would be necessary to intercept the Detectron2 messages on their way to its logger and share them with AML. I have since figured out how to do that, but at the time, I just wrote a bit of code to visualize both the output and the learning rate schedule once the run was finished. I then experimented quite actively with learning rate schedules. I found that I had misinterpreted some of Detectron2’s learning rate schedule parameters and the initial learning rate was too high. I eventually figured out where to aim (although it was a moving target, because the best rate dropped as the model improved), and training finally started going well. . Dropping the ‘boma’ class and re-training . In the US, a corral for livestock is usually made with fence posts and wire, but in subSaharan Africa, corrals are often made by piling up brush in a rough circle or rectangle. In Tanzania, those brush corrals are called ‘bomas’. Howard wanted to be able to identify bomas, and they were included in our object list. As the training proceeded, the model accuracy got up to 80-90% for most of the important classes, but there were still an unacceptable number of false-positives. Looking at the output, we quickly realized that most of the problem was in the ‘boma’ class. The model obviously recognized straight edges, but sometimes got the scale very wrong because at smaller scales, everything looks like a boma. We didn’t have a very big sample size of labeled bomas, and we decided that it was better to leave the class for another time. So we dropped it and retrained, starting by training the ROI heads with the rest of the model layers frozen. That fixed the false-positive problem and the accuracy got quite good. It’s currently above 95% for the common categories, with only 5 - 7% false-positives. The 05_aml_pipeline.ipynb notebook describes the training. . TridentNet’s three heads . TridentNet has an unusual feature from which its name is derived: the model has three separate heads that handle small, medium, and large objects, respectively. The TridentNet creators wrote that once the model was trained, it worked perfectly well to do inference using only the medium head, to save computation. We compared inference using all of the heads with inference using only the medium head and so far, I agree with their conclusion. . Uh oh, how do you share results? . Another unanticipated challenge cropped up at this point: how do you share results with colleagues? It hadn’t occurred to me that it would be a problem. There were two major problems: first, I was turning each full-sized image into 54 tiles and running the tiles through the model. But would a colleague really want to see 54 different annotations per original image? I thought not, so I decided to re-combine the annotations (done separately for Detectron2 output and the fastai classification model output). . Secondly, the output from a Detectron2 model is a Python dict containing Detectron2 classes, so you need Detectron2 installed to view the annotations. Installing Detectron2 is non-trivial, and even if I could translate the annotations to a simpler format, most people don’t have access to software that will superimpose bounding boxes onto an image. So as a stopgap measure, I wrote some code to do the superposition myself, and then shared image pairs consisting of a full-sized, annotated image plus its corresponding original image with my colleagues. It’s not a sustainable solution because the files are so large, so I am really just highlighting the need to think about how you are going to share results with your colleagues. .",
            "url": "https://jcpayne.github.io/aerial-survey-blog/2021/02/11/training-and-sharing-results.html",
            "relUrl": "/2021/02/11/training-and-sharing-results.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Building the TridentNet model",
            "content": "Assembling training and validation sets . I learned the hard way that the most difficult part of an AI project is assembling the training and validation datasets. Why? Because you will almost certainly be assembling images and annotations from a variety of sources in order to get a large enough sample size, a good balance between categories, and as large a diversity as possible of images– including backgrounds, lighting, perspectives, and so on. The images and annotations must match, or at a minimum, it has to be clear which images have been checked. You will be astounded at how many things can go wrong; it is truly the Land of Edge Cases – there was even one time where the images I received had been tiled to a different size, so I had to merge the input tiles back into full-size images before re-tiling them. In short, I failed to insist on a high level of organization right off the bat, and I paid for it. I make some recommendations for how to do it better in the 03_process_annotations.ipynb notebook. . Building the TridentNet model . Getting the TridentNet model to run was slow because Detectron2 has a variety of different implementations including bounding-box models, instance segmentation, keypoints, and so on, documentation is quite limited, and not that many people seem to use it (or perhaps they all knew what they’re doing, unlike me). Detectron2 has its own environment that includes ‘registered’ datasets, custom data loaders, multi-GPU management, internal logging, and so on. The default training cycle was a bit clunkier than I wanted, and I realized early on that I’d need additional image augmentation, given that my sample size was relatively small. It took time to figure out how to get Detectron2 training efficiently on multiple GPUs, and I added some other customizations, including fixing a routine that helped to balance the sample size across categories, bypassing an evaluation routine, passing in-memory tiles to a custom dataloader, adding an Adabound optimizer, and others. Model customizations are in trident_project/dev_packages/trident_dev/model.py). . Learning to tile . The current generation of GPUs can’t handle full-sized images from modern cameras, so the first step in preparing an image for an AI model is almost always to resize it, making it much smaller and sacrificing a lot of resolution. If the object of interest fills a large proportion of the image area, then that approach works perfectly well. But in an aerial survey of livestock or wildlife, a small object like a goat or gazelle might only occupy 30 pixels in an image that could contain tens of millions of pixels. That means we can’t afford to resize the image and lose resolution. The standard solution is to chop it into pieces, or ‘tiles’. . Tiling the training data . I still haven’t found good workflows for tiling, but I found a package that did some of what I wanted and forked it (my version is still rough; it’s trident_project/dev_packages/image_bbox_tiler.py). . In the process of building a training set of tiles from a set of full-size images and annotations, I uncovered a problem I hadn’t anticipated, which was that when you chop a bounding box into pieces, you may end up with little fragments that you don’t necessarily want to keep. I figured out a set of rules for deciding which to drop. Those are described in the 04_tile_training_images.ipynb notebook. . I also revised the tilling package to omit most of the empty tiles, but to randomly sample a small percentage of them. Having a good selection of empty tiles is important to training a model when 95% of the tiles that the model will see during inference are expected to be empty. . Passing in-memory tiles to a dataloader . Writing tiles to disk is a very slow and resource-intensive process, so I decided to try holding the tiles in memory and passing them directly to the model. That turned out to be quite complicated because AI modeling code always expects to have the same number of annotations and images. In brief, I ended up creating a new dataloader for the tiles from each full-size image. It works well for a single GPU and for distributed training, but I still don’t have it set up quite right for distributed inference because Detectron2 spawns processes deep in the codebase in a way that I haven’t been able to fully understand. I’ve managed to increase the inference speed by 2X with 4 GPUs, but it should be roughly 4X faster, so I’m still missing something. .",
            "url": "https://jcpayne.github.io/aerial-survey-blog/2020/07/10/building-the-model.html",
            "relUrl": "/2020/07/10/building-the-model.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Getting started, and some things I wish I had known",
            "content": "Finding a model . As we began the project, the first question was “Which model should we use?” I’ve never participated in a scientific field that is moving as fast as AI is. It is nearly a full-time job just to stay up to date with research. So I read a pile of papers and settled on TridentNet, a recent variant of a model called Detectron2 that was created by Facebook Research. I figured it would be yesterday’s news soon enough, but it seemed to have the right balance between achieving state-of-the-art results and being mature enough to make deployment possible. . Getting started in the cloud . Microsoft’s cloud computing architecture, Azure, like Amazon’s AWS or the Google Cloud, has a huge variety of offerings aimed at everyone from individuals to the multinational companies that are the world’s heaviest data users. The AI4E program was very helpful in getting me oriented to Azure’s machine learning offerings. The lessons I learned were: . Use a Data Science Virtual Machine to start with. It’s a VM that has one or more GPUs and comes with most of the libraries pre-installed that one needs for machine learning. VM’s are great for developing code (and Google’s Colab is even easier). Treat VMs as disposable. Python libraries seem to get outdated and corrupted over time if you are doing heavy development, and it’s nice to be able to start again with a fresh machine. It makes sense to add an SSD data disk, but you’ll discover that storage is one of the largest costs on your account. Blob storage is cheaper but not as convenient, and it’s not really worth switching to blob storage unless a) you have a vast amount of data or b) you are working with an AML workspace. | I work in Python and Pytorch, and I use the wonderful interactive Jupyter Notebook ecosystem (Jupyterlab, to be specific) to develop code. However, Jupyterlab occasionally introduces bugs or incompatibilities and it certainly slows code down, so once your program is working and you need it to run fast, you’re better off calling Python code from the command line. I also use a special package called nbdev that makes it easy to develop in a Jupyter notebook and then compile the code as .py files. However, nbdev requires you to work in a git repository that is set up with a special template, and getting used to its workflow requires a signficant mind shift. The best general editor (really more of an IDE) that I’ve found is Microsoft’s Visual Studio Code. It can be used on your local machine and also can connect to a remote headless VM. Supposedly, it can even be used to debug code running in a container that is running on a remote VM, but I haven’t tried the latter (many dense and forbidding pages of instructions in Azure). At a minimum, it’s a good way to explore code. | Once I had gotten a model running, I gradually discovered the benefits of using an Azure Machine Learning Workspace. It required quite a bit of work to learn how to use the Python SDK (azureml-sdk) and even more work to get comfortable with building a containerized Docker environment, but it paid dividends when I needed to scale up for heavy training or inference. AML was brand new and very buggy when I started, but it has since settled down. The documentation is good. The 05_aml_pipeline.ipynb notebook shows how I used it, and the Dockerfile I built is at trident_project/docker_files in the repository. | Deployment is a specialty. There are people called “DevOps engineers” who do nothing but figure out how to scale up software. I recommend easing yourself gradually into it; don’t assume that you need it at first. It’s pretty amazing what you can accomplish with a single, cheap VM. Also ask yourself seriously what your endpoint is. Do you really need to maintain a website with a backend model and database infrastructure, or can you get by sharing a VM with teammates, or producing batch results from time to time? When you eventually do start to need something bigger, Azure has all of the tools you’ll need to run multiple containers with Kubernetes, to recover from interrupted jobs, distribute your data, build pipelines, expose web endpoints, and so on (ditto for Amazon and Google, of course). | AML accounts and permissions are a big headache, just like with AWS. I’ve had to switch between accounts, copy data and remake virtual machines at least 6 times, and have also been shut out of accounts for a total of more than two months over the last two years by a combination of issues that included some serious technical support problems. That’s a lot of wasted time, and makes you yearn for the simplicity of working on a local machine. But working in the cloud is inevitable and Azure’s services are as good as any. | An AML workspace integrates a lot of different machine learning functionality (image from Microsoft) . Keeping track of code . As I built one VM after another and switched back and forth between accounts, I started realizing that I had a seriously tangled mess of code to keep track of. My primary need was to keep the versions of code in sync between all of the machines I worked on, which included a laptop and 3 or 4 remote VMs on different accounts. Working with collaborators was a secondary concern. I fell back on git and Github, the world standard. Git is superb but it is not simple; as someone once said, you never really stop learning how to use it. My solution was to create a private Github repository for keeping my own mess straight, sync it with each of the remote machines, and then push a clean master copy from that repository to another repository that I shared with Howard. It’s been a great system, but I have to remember to always pull from the central repository before beginning work on a new machine, and to push to it before shutting down a machine. . My git workflow .",
            "url": "https://jcpayne.github.io/aerial-survey-blog/2020/04/01/getting-started.html",
            "relUrl": "/2020/04/01/getting-started.html",
            "date": " • Apr 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m jus’ zis guy, you know? . This website is powered by fastpages [^1]. [^1]:a blogging platform that natively supports Jupyter notebooks in addition to other formats. .",
          "url": "https://jcpayne.github.io/aerial-survey-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jcpayne.github.io/aerial-survey-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}